{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook for working with Clever data\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "storage_account = 'sttesteduanalyticscisd3'\n",
        "\n",
        "stage1 = f'abfss://stage1@{storage_account}.dfs.core.windows.net'\n",
        "stage2 = f'abfss://stage2@{storage_account}.dfs.core.windows.net'\n",
        "stage3 = f'abfss://stage3@{storage_account}.dfs.core.windows.net'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Process resource usage\n",
        "df = spark.read.csv(stage1 + '/clever', header='true', inferSchema='true')\n",
        "df = df.withColumn('sis_id',df.sis_id.cast('string'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage2}/clever/resource_usage_students')\n",
        "df.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Anonymize data and load into stage3\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "df = spark.read.format('parquet').load(f'{stage2}/clever/resource_usage_students')\n",
        "df = df.withColumn('sis_id', sha2(df.sis_id, 256)).withColumn('clever_user_id',lit('*')).withColumn('clever_school_id',lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/clever/resource_usage_students')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sql on-demand db for Clever data\n",
        "\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
        "    spark.sql(f\"create table if not exists {db_name}.comprehensive_student_lesson_activity_with_standards_ela using PARQUET location '{source_path}/resource_usage_students'\")\n",
        "\n",
        "# Drop all tables in a db, then drop the db\n",
        "def drop_db(db_name):\n",
        "    df = spark.sql('SHOW TABLES FROM ' + db_name)\n",
        "    for row in df.rdd.collect():\n",
        "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{row['tableName']}\")\n",
        "    spark.sql(f\"DROP DATABASE {db_name}\")    \n",
        "\n",
        "#drop_db('s2_iready')\n",
        "create_spark_db('s2_clever', f'{stage2}/clever')\n",
        "#drop_db('s3_iready')\n",
        "create_spark_db('s3_clever', f'{stage3}/clever')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "sessionOptions": {
      "driverMemory": "28g",
      "driverCores": 4,
      "executorMemory": "28g",
      "executorCores": 4,
      "numExecutors": 2,
      "keepAliveTimeout": 30,
      "conf": {
        "spark.dynamicAllocation.enabled": "false",
        "spark.dynamicAllocation.minExecutors": "2",
        "spark.dynamicAllocation.maxExecutors": "2"
      }
    },
    "saveOutput": true,
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}