{
  "metadata": {
    "sessionOptions": {
      "driverMemory": "28g",
      "driverCores": 4,
      "executorMemory": "28g",
      "executorCores": 4,
      "numExecutors": 2,
      "keepAliveTimeout": 30,
      "conf": {
        "spark.dynamicAllocation.enabled": "false",
        "spark.dynamicAllocation.minExecutors": "2",
        "spark.dynamicAllocation.maxExecutors": "2"
      }
    },
    "saveOutput": true,
    "language_info": {
      "name": "python"
    },
    "a365ComputeOptions": {
      "name": "spark1",
      "id": "/subscriptions/17be3a95-2f9c-4c24-80cc-3974d6021e31/resourceGroups/EduAnalytics/providers/Microsoft.Synapse/workspaces/syeduanalyticscisd/bigDataPools/spark1",
      "endpoint": "https://syeduanalyticscisd.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark1",
      "type": "Spark",
      "auth": {
        "authResource": "https://dev.azuresynapse.net",
        "type": "AAD"
      },
      "sparkVersion": "2.4",
      "nodeCount": 10,
      "nodeSize": "Small",
      "automaticScaleJobs": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenEduAnalytics\n",
        "This notebook is for creating a consolidated view over the data from each of the source systems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "storage_account = 'saeduanalytics'\n",
        "\n",
        "stage1 = f'abfss://m365share@{storage_account}.dfs.core.windows.net'\n",
        "stage2 = f'abfss://stage2@{storage_account}.dfs.core.windows.net'\n",
        "stage3 = f'abfss://stage3@{storage_account}.dfs.core.windows.net'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Process sectionmark.csv data (from stage1 into stage2 and stage3)\n",
        "# Convert id values to use the Person.Id and Section.Id values set in the Education Data Platform.\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "#df = spark.read.csv(f'{stage1_additional_path}/studentsectionmark.csv', header='true')\n",
        "#sqlContext.registerDataFrameAsTable(df, 'SectionMark')\n",
        "\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/datasense/studentsectionmark'), 'SectionMark')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Person'), 'Person')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Section'), 'Section')\n",
        "\n",
        "df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\n",
        "sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\n",
        "sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\n",
        "from SectionMark sm, Person p, Section s \\\n",
        "where sm.student_id = p.ExternalId \\\n",
        "and sm.section_id = s.ExternalId\")\n",
        "\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage2}/OpenEduAnalytics/SectionMark')\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage2}/OpenEduAnalytics/SectionMark2')\n",
        "\n",
        "# Add SectionMark data to stage3 (anonymized parquet lake)\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/OpenEduAnalytics/SectionMark')\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/OpenEduAnalytics/SectionMark2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Process studentattendance.csv data (from stage1 into stage2 and stage3)\n",
        "# Convert id values to use the Person.Id, Org.Id and Section.Id values set in the Education Data Platform.\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "\n",
        "#df = spark.read.csv(f'{stage1_additional_path}/studentattendance.csv', header='true')\n",
        "#sqlContext.registerDataFrameAsTable(df, 'Attendance')\n",
        "\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/datasense/studentattendance'), 'Attendance')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Org'), 'Org')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Person'), 'Person')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Section'), 'Section')\n",
        "df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, to_date(att.attendance_date,'MM/dd/yyyy') AttendanceDate, \\\n",
        "att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\n",
        "att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\n",
        "from Attendance att, Org o, Person p, Section s \\\n",
        "where att.student_id = p.ExternalId \\\n",
        "and att.school_id = o.ExternalId \\\n",
        "and att.section_id = s.ExternalId\")\n",
        "\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage2}/OpenEduAnalytics/Attendance')\n",
        "\n",
        "# Add Attendance data to stage3 (anonymized parquet lake)\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/OpenEduAnalytics/Attendance')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "outputCollapsed": true
      },
      "source": [
        "# Add 'Department' column to Course in edu_dl\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Course'), 'Course')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/datasense/course'), 'dsCourse')\n",
        "\n",
        "df = spark.sql(\"select c1.Id, c1.Name, c1.Code, c1.Description, c1.ExternalId, c1.CreateDate, \\\n",
        "c1.LastModifiedDate, c1.IsActive, c1.CalendarId, c2.department Department \\\n",
        "from Course c1, dsCourse c2 \\\n",
        "where c1.ExternalId = c2.id\")\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage2}/OpenEduAnalytics/Course')\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/OpenEduAnalytics/Course')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
        "    spark.sql(f\"create table if not exists {db_name}.Activity using PARQUET location '{source_path}/m365/Activity'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Calendar using PARQUET location '{source_path}/m365/Calendar'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Org using PARQUET location '{source_path}/m365/Org'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Person using PARQUET location '{source_path}/m365/Person'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.PersonIdentifier using PARQUET location '{source_path}/m365/PersonIdentifier'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.RefDefinition using PARQUET location '{source_path}/m365/RefDefinition'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Section using PARQUET location '{source_path}/m365/Section'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Session using PARQUET location '{source_path}/m365/Session'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StaffOrgAffiliation using PARQUET location '{source_path}/m365/StaffOrgAffiliation'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StaffSectionMembership using PARQUET location '{source_path}/m365/StaffSectionMembership'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StudentOrgAffiliation using PARQUET location '{source_path}/m365/StudentOrgAffiliation'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StudentSectionMembership using PARQUET location '{source_path}/m365/StudentSectionMembership'\")\n",
        "\n",
        "    spark.sql(f\"create table if not exists {db_name}.Course using PARQUET location '{source_path}/OpenEduAnalytics/Course'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Attendance using PARQUET location '{source_path}/OpenEduAnalytics/Attendance'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.SectionMark using PARQUET location '{source_path}/OpenEduAnalytics/SectionMark'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.SectionMark2 using PARQUET location '{source_path}/OpenEduAnalytics/SectionMark2'\")\n",
        "\n",
        "create_spark_db('s2_OpenEduAnalytics', stage2)\n",
        "create_spark_db('s3_OpenEduAnalytics', stage3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Drop all tables in a db, then drop the db\n",
        "def drop_db(db_name):\n",
        "    df = spark.sql('SHOW TABLES FROM ' + db_name)\n",
        "    for row in df.rdd.collect():\n",
        "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{row['tableName']}\")\n",
        "    spark.sql(f\"DROP DATABASE {db_name}\")\n",
        "\n",
        "drop_db('db_name')"
      ]
    }
  ]
}