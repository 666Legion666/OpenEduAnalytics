{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Calendar table creation\r\n",
        "\r\n",
        "This notebook creates a basic calendar table to support data analysis in a Power BI dashboard.\r\n",
        "\r\n",
        "### Tables used:\r\n",
        "- None\r\n",
        "\r\n",
        "### Tables created\r\n",
        "- Spark DB: ds3_main (stage 3 data science main)\r\n",
        "    - Calendar\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# data lake and container information\n",
        "storage_account = 'saeduanalytics'\n",
        "stage1 = f'abfss://stage1@{storage_account}.dfs.core.windows.net'\n",
        "stage2 = f'abfss://stage2@{storage_account}.dfs.core.windows.net'\n",
        "stage3 = f'abfss://stage3@{storage_account}.dfs.core.windows.net'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 524,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-20T15:32:19.4774364Z",
              "session_start_time": "2021-07-20T15:32:19.5087192Z",
              "execution_start_time": "2021-07-20T15:33:32.2621857Z",
              "execution_finish_time": "2021-07-20T15:33:34.3228804Z"
            },
            "text/plain": "StatementMeta(spark1, 524, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# date range\r\n",
        "start = \"2020-01-01\"\r\n",
        "stop = \"2021-12-30\"\r\n",
        "\r\n",
        "# create calendar dataframe\r\n",
        "temp_df = spark.createDataFrame([(start, stop)], (\"start\", \"stop\"))\r\n",
        "temp_df = temp_df.select([col(c).cast(\"timestamp\") for c in (\"start\", \"stop\")])\r\n",
        "temp_df = temp_df.withColumn(\"stop\",date_add(\"stop\",1).cast(\"timestamp\"))\r\n",
        "temp_df = temp_df.select([col(c).cast(\"long\") for c in (\"start\", \"stop\")])\r\n",
        "start, stop = temp_df.first()\r\n",
        "interval=60*60*24\r\n",
        "\r\n",
        "df = spark.range(start,stop,interval).select(col(\"id\").cast(\"timestamp\").alias(\"DateTime\"))\r\n",
        "df = df.withColumn(\"Date\", to_date(col(\"DateTime\")))\r\n",
        "\r\n",
        "df = df.drop(\"DateTime\")\r\n",
        "df = df.withColumn('Year', date_format('Date', 'YYYY'))\r\n",
        "df = df.withColumn('Month', date_format('Date', 'MMMM'))\r\n",
        "df = df.withColumn('MonthNum', date_format('Date', 'M'))\r\n",
        "df = df.withColumn('Week', date_format('Date', 'W'))\r\n",
        "df = df.withColumn('Day', date_format('Date', 'D'))\r\n",
        "\r\n",
        "df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 524,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-20T15:35:40.5870542Z",
              "session_start_time": null,
              "execution_start_time": "2021-07-20T15:35:40.6894885Z",
              "execution_finish_time": "2021-07-20T15:35:42.7434772Z"
            },
            "text/plain": "StatementMeta(spark1, 524, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+-------+--------+----+---+\n",
            "|      Date|Year|  Month|MonthNum|Week|Day|\n",
            "+----------+----+-------+--------+----+---+\n",
            "|2020-01-01|2020|January|       1|   1|  1|\n",
            "|2020-01-02|2020|January|       1|   1|  2|\n",
            "|2020-01-03|2020|January|       1|   1|  3|\n",
            "|2020-01-04|2020|January|       1|   1|  4|\n",
            "|2020-01-05|2020|January|       1|   2|  5|\n",
            "|2020-01-06|2020|January|       1|   2|  6|\n",
            "|2020-01-07|2020|January|       1|   2|  7|\n",
            "|2020-01-08|2020|January|       1|   2|  8|\n",
            "|2020-01-09|2020|January|       1|   2|  9|\n",
            "|2020-01-10|2020|January|       1|   2| 10|\n",
            "|2020-01-11|2020|January|       1|   2| 11|\n",
            "|2020-01-12|2020|January|       1|   3| 12|\n",
            "|2020-01-13|2020|January|       1|   3| 13|\n",
            "|2020-01-14|2020|January|       1|   3| 14|\n",
            "|2020-01-15|2020|January|       1|   3| 15|\n",
            "|2020-01-16|2020|January|       1|   3| 16|\n",
            "|2020-01-17|2020|January|       1|   3| 17|\n",
            "|2020-01-18|2020|January|       1|   3| 18|\n",
            "|2020-01-19|2020|January|       1|   4| 19|\n",
            "|2020-01-20|2020|January|       1|   4| 20|\n",
            "+----------+----+-------+--------+----+---+\n",
            "only showing top 20 rows"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Data Back to Lake"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write back to the lake in stage 3 ds3_main directory\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/ds3_main/Calendar')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 524,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-20T15:36:01.4806236Z",
              "session_start_time": null,
              "execution_start_time": "2021-07-20T15:36:01.5908837Z",
              "execution_finish_time": "2021-07-20T15:36:13.9808153Z"
            },
            "text/plain": "StatementMeta(spark1, 524, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load to Spark DB"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create spark db to allow for access to the data in the data lake via SQL on-demand.\r\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\r\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\r\n",
        "def create_spark_db(db_name, source_path):\r\n",
        "    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
        "    spark.sql(f\"DROP TABLE IF EXISTS {db_name}.Calendar\")\r\n",
        "    spark.sql(f\"create table if not exists {db_name}.Calendar using PARQUET location '{source_path}/Calendar'\")\r\n",
        "\r\n",
        "create_spark_db('ds3_main', stage3 + '/ds3_main')"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}