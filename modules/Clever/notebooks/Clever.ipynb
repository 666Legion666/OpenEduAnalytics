{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook for working with Clever data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "storage_account = 'steduanalyticstestx1'\n",
        "\n",
        "stage1 = f'abfss://test-data@{storage_account}.dfs.core.windows.net/stage1'\n",
        "#stage1 = f'abfss://stage1@{storage_account}.dfs.core.windows.net'\n",
        "stage2 = f'abfss://stage2@{storage_account}.dfs.core.windows.net'\n",
        "stage3 = f'abfss://stage3@{storage_account}.dfs.core.windows.net'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Process resource usage\n",
        "df = spark.read.csv(stage1 + '/clever', header='true', inferSchema='true')\n",
        "df = df.withColumn('sis_id',df.sis_id.cast('string'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/clever/resource_usage_students')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Anonymize data and load into stage3\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "df = spark.read.format('parquet').load(stage2 + '/clever/resource_usage_students')\n",
        "df = df.withColumn('sis_id', sha2(df.sis_id, 256)).withColumn('clever_user_id',lit('*')).withColumn('clever_school_id',lit('*'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/clever/resource_usage_students')\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create sql on-demand db for Clever data\n",
        "\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql('CREATE DATABASE IF NOT EXISTS ' + db_name)\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".resource_usage_students using PARQUET location '\" + source_path + \"/resource_usage_students'\")\n",
        "\n",
        "# Drop all tables in a db, then drop the db\n",
        "def drop_db(db_name):\n",
        "    df = spark.sql('SHOW TABLES FROM ' + db_name)\n",
        "    for row in df.rdd.collect():\n",
        "        spark.sql(\"DROP TABLE IF EXISTS \" + db_name + \".\" + row['tableName'])\n",
        "    spark.sql(\"DROP DATABASE \" + db_name)    \n",
        "\n",
        "#drop_db('s2_clever')\n",
        "create_spark_db('s2_clever', stage2 + '/clever')\n",
        "#drop_db('s3_clever')\n",
        "create_spark_db('s3_clever', stage3 + '/clever')"
      ]
    }
  ]
}