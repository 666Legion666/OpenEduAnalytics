{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tests for processing SIF data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "11",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-27T20:27:17.9438201Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-27T20:27:18.5878544Z",
              "execution_finish_time": "2022-11-27T20:27:18.5882201Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(, 11, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-27 20:27:18,357 - OEA - INFO - Now using workspace: dev\n2022-11-27 20:27:18,358 - OEA - INFO - OEA initialized.\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace('sam')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2sm",
              "session_id": "11",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-27T20:27:20.4094314Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-27T20:27:20.5475565Z",
              "execution_finish_time": "2022-11-27T20:27:20.7300643Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2sm, 11, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-27 20:27:20,540 - OEA - INFO - Now using workspace: sam\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Now land a batch data file into stage1 of the data lake.\r\n",
        "# In this example we pull a test csv data file from github and it is landed in oea/sandboxes/sam/stage1/Transactional/contoso/v0.1/students/delta_batch_data/rundate=<utc datetime>\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/azure-data-services-go-fast-codebase/main/solution/SampleFiles/sif/StudentPersonal.json').text\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/azure-data-services-go-fast-codebase/main/solution/SampleFiles/sif/StudentDailyAttendance.json').text\r\n",
        "oea.land(data, 'sif/v3.4.9/StudentPersonal', 'StudentPersonal.json', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "oea.land(data, 'sif/v3.4.9/StudentDailyAttendance', 'StudentDailyAttendance.json', oea.SNAPSHOT_BATCH_DATA)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2sm",
              "session_id": "11",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-27T20:29:14.680611Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-27T20:29:14.7994621Z",
              "execution_finish_time": "2022-11-27T20:29:15.933993Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2sm, 11, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "'stage1/Transactional/sif/v3.4.9/StudentDailyAttendance/snapshot_batch_data/rundate=2022-11-27 20-29-15/StudentDailyAttendance.json'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) The next step is to ingest the batch data into stage2\r\n",
        "# Note that when you run this the first time, you'll see an info message like \"Number of new inbound rows processed: 2\".\r\n",
        "# If you run this a second time, the number of inbound rows processed will be 0 because the ingestion uses spark structured streaming to keep track of what data has already been processed.\r\n",
        "options = {'multiline':True}\r\n",
        "oea.ingest(f'sif/v3.4.9/StudentPersonal', 'RefId', options)\r\n",
        "oea.ingest(f'sif/v3.4.9/StudentDailyAttendance', 'RefId', options)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Now you can run queries against the auto-generated \"lake database\" with the ingested sif data.\r\n",
        "df = spark.sql(\"select * from ldb_sam_s2i_sif_v3p4p9.studentpersonal\")\r\n",
        "display(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to reset this example (deleting all the example sif data in your workspace)\r\n",
        "oea.rm_if_exists('stage1/Transactional/sif')\r\n",
        "oea.rm_if_exists('stage2/Ingested/sif')\r\n",
        "oea.rm_if_exists('stage2/Refined/sif')\r\n",
        "oea.drop_lake_db('ldb_sam_s2i_sif_v3p4p9')\r\n",
        "oea.drop_lake_db('ldb_sam_s2r_sif_v3p4p9')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2sm",
              "session_id": "11",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-27T20:28:48.5459422Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-27T20:28:48.6519541Z",
              "execution_finish_time": "2022-11-27T20:29:05.5324749Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2sm, 11, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-27 20:29:03,912 - OEA - INFO - Database dropped: ldb_sam_s2i_sif_v3p4p9\n2022-11-27 20:29:04,039 - OEA - INFO - Database dropped: ldb_sam_s2r_sif_v3p4p9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "'Database dropped: ldb_sam_s2r_sif_v3p4p9'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate an initial metadata file for manual modification\r\n",
        "metadata = oea.create_metadata_from_lake_db('ldb_sam_s2i_sif_v3p4p9')\r\n",
        "dlw = DataLakeWriter(oea.to_url('stage1/Transactional/sif'))\r\n",
        "dlw.write('metadata.csv', metadata)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2sm",
              "session_id": "11",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-27T20:30:44.5639634Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-27T20:30:44.6737968Z",
              "execution_finish_time": "2022-11-27T20:30:46.5340236Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2sm, 11, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sql db for the ingested SIF data\r\n",
        "oea.create_sql_db('stage2/Ingested/SIF')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tests for processing SIF data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": "11",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-27T20:27:17.9438201Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-27T20:27:18.5878544Z",
              "execution_finish_time": "2022-11-27T20:27:18.5882201Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(, 11, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-27 20:27:18,357 - OEA - INFO - Now using workspace: dev\n2022-11-27 20:27:18,358 - OEA - INFO - OEA initialized.\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace('sam')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2sm",
              "session_id": "11",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-27T20:27:20.4094314Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-27T20:27:20.5475565Z",
              "execution_finish_time": "2022-11-27T20:27:20.7300643Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2sm, 11, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-27 20:27:20,540 - OEA - INFO - Now using workspace: sam\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Now land a batch data file into stage1 of the data lake.\r\n",
        "# In this example we pull a test csv data file from github and it is landed in oea/sandboxes/sam/stage1/Transactional/contoso/v0.1/students/delta_batch_data/rundate=<utc datetime>\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/azure-data-services-go-fast-codebase/main/solution/SampleFiles/sif/StudentPersonal.json').text\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/azure-data-services-go-fast-codebase/main/solution/SampleFiles/sif/StudentDailyAttendance.json').text\r\n",
        "oea.land(data, 'sif/v3.4.9/StudentPersonal', 'StudentPersonal.json', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "oea.land(data, 'sif/v3.4.9/StudentDailyAttendance', 'StudentDailyAttendance.json', oea.SNAPSHOT_BATCH_DATA)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2sm",
              "session_id": "11",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-27T20:29:14.680611Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-27T20:29:14.7994621Z",
              "execution_finish_time": "2022-11-27T20:29:15.933993Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2sm, 11, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "'stage1/Transactional/sif/v3.4.9/StudentDailyAttendance/snapshot_batch_data/rundate=2022-11-27 20-29-15/StudentDailyAttendance.json'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) The next step is to ingest the batch data into stage2\r\n",
        "# Note that when you run this the first time, you'll see an info message like \"Number of new inbound rows processed: 2\".\r\n",
        "# If you run this a second time, the number of inbound rows processed will be 0 because the ingestion uses spark structured streaming to keep track of what data has already been processed.\r\n",
        "options = {'multiline':True}\r\n",
        "oea.ingest(f'sif/v3.4.9/StudentPersonal', 'RefId', options)\r\n",
        "oea.ingest(f'sif/v3.4.9/StudentDailyAttendance', 'RefId', options)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Now you can run queries against the auto-generated \"lake database\" with the ingested sif data.\r\n",
        "df = spark.sql(\"select * from ldb_sam_s2i_sif_v3p4p9.studentpersonal\")\r\n",
        "display(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to reset this example (deleting all the example sif data in your workspace)\r\n",
        "oea.rm_if_exists('stage1/Transactional/sif')\r\n",
        "oea.rm_if_exists('stage2/Ingested/sif')\r\n",
        "oea.rm_if_exists('stage2/Refined/sif')\r\n",
        "oea.drop_lake_db('ldb_sam_s2i_sif_v3p4p9')\r\n",
        "oea.drop_lake_db('ldb_sam_s2r_sif_v3p4p9')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2sm",
              "session_id": "11",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-27T20:28:48.5459422Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-27T20:28:48.6519541Z",
              "execution_finish_time": "2022-11-27T20:29:05.5324749Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2sm, 11, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-27 20:29:03,912 - OEA - INFO - Database dropped: ldb_sam_s2i_sif_v3p4p9\n2022-11-27 20:29:04,039 - OEA - INFO - Database dropped: ldb_sam_s2r_sif_v3p4p9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "'Database dropped: ldb_sam_s2r_sif_v3p4p9'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate an initial metadata file for manual modification\r\n",
        "metadata = oea.create_metadata_from_lake_db('ldb_sam_s2i_sif_v3p4p9')\r\n",
        "dlw = DataLakeWriter(oea.to_url('stage1/Transactional/sif'))\r\n",
        "dlw.write('metadata.csv', metadata)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark3p2sm",
              "session_id": "11",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-11-27T20:30:44.5639634Z",
              "session_start_time": null,
              "execution_start_time": "2022-11-27T20:30:44.6737968Z",
              "execution_finish_time": "2022-11-27T20:30:46.5340236Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(spark3p2sm, 11, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sql db for the ingested SIF data\r\n",
        "oea.create_sql_db('stage2/Ingested/SIF')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
