{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GraphAPI_py\r\n",
        "\r\n",
        "This OEA Graph API Module python class provides:\r\n",
        "- Data schema definitions\r\n",
        "- Data pseudonomization settings\r\n",
        "- Data processing for Stage 1np data to Stage 2p and 2np\r\n",
        "  * Flattening of JSON files into an expected table format"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\r\n",
        "\r\n",
        "class GraphAPI(BaseOEAModule):\r\n",
        "    def __init__(self, source_folder='graph_api'):\r\n",
        "        BaseOEAModule.__init__(self, source_folder)\r\n",
        "\r\n",
        "        self.stage1np_graphapi_users = self.stage1np + '/users'\r\n",
        "        self.stage1np_graphapi_m365 = self.stage1np + '/m365_app_user_detail'\r\n",
        "        self.stage1np_graphapi_teams = self.stage1np + '/teams_activity_user_detail'\r\n",
        "        self.stage1np_graphapi_meetings = self.stage1np + '/meeting_attendance_report'\r\n",
        "\r\n",
        "        self.schemas['users'] = [['surname', 'string', 'mask'],\r\n",
        "                                ['givenName', 'string', 'mask'],\r\n",
        "                                ['userPrincipalName', 'string', 'hash'],\r\n",
        "                                ['id', 'string', 'mask'],\r\n",
        "                                ['reportYearMonth', 'string', 'partition-by']]\r\n",
        "\r\n",
        "        self.schemas['m365'] = [['reportRefreshDate', 'date', 'no-op'],\r\n",
        "                                ['userPrincipalName', 'string', 'hash'],\r\n",
        "                                ['lastActivationDate', 'date', 'no-op'],\r\n",
        "                                ['lastActivityDate', 'date', 'no-op'],\r\n",
        "                                ['reportPeriod', 'string', 'no-op'],\r\n",
        "                                ['mobile', 'boolean', 'no-op'],\r\n",
        "                                ['web', 'boolean', 'no-op'],\r\n",
        "                                ['mac', 'boolean', 'no-op'],\r\n",
        "                                ['windows', 'boolean', 'no-op'],\r\n",
        "                                ['excel', 'boolean', 'no-op'],\r\n",
        "                                ['excelMac', 'boolean', 'no-op'],\r\n",
        "                                ['excelMobile', 'boolean', 'no-op'],\r\n",
        "                                ['excelWeb', 'boolean', 'no-op'],\r\n",
        "                                ['excelWindows', 'boolean', 'no-op'],\r\n",
        "                                ['oneNote', 'boolean', 'no-op'],\r\n",
        "                                ['oneNoteMac', 'boolean', 'no-op'],\r\n",
        "                                ['oneNoteMobile', 'boolean', 'no-op'],\r\n",
        "                                ['oneNoteWeb', 'boolean', 'no-op'],\r\n",
        "                                ['oneNoteWindows', 'boolean', 'no-op'],\r\n",
        "                                ['outlook', 'boolean', 'no-op'],\r\n",
        "                                ['outlookMac', 'boolean', 'no-op'],\r\n",
        "                                ['outlookMobile', 'boolean', 'no-op'],\r\n",
        "                                ['outlookWeb', 'boolean', 'no-op'],\r\n",
        "                                ['outlookWindows', 'boolean', 'no-op'],\r\n",
        "                                ['powerPoint', 'boolean', 'no-op'],\r\n",
        "                                ['powerPointMac', 'boolean', 'no-op'],\r\n",
        "                                ['powerPointMobile', 'boolean', 'no-op'],\r\n",
        "                                ['powerPointWeb', 'boolean', 'no-op'],\r\n",
        "                                ['powerPointWindows', 'boolean', 'no-op'],\r\n",
        "                                ['teams', 'boolean', 'no-op'],\r\n",
        "                                ['teamsMac', 'boolean', 'no-op'],\r\n",
        "                                ['teamsMobile', 'boolean', 'no-op'],\r\n",
        "                                ['teamsWeb', 'boolean', 'no-op'],\r\n",
        "                                ['teamsWindows', 'boolean', 'no-op'],\r\n",
        "                                ['word', 'boolean', 'no-op'],\r\n",
        "                                ['wordMac', 'boolean', 'no-op'],\r\n",
        "                                ['wordMobile', 'boolean', 'no-op'],\r\n",
        "                                ['wordWeb', 'boolean', 'no-op'],\r\n",
        "                                ['wordWindows', 'boolean', 'no-op'],\r\n",
        "                                ['reportYearMonth', 'string', 'partition-by']]\r\n",
        "\r\n",
        "        self.schemas['teams'] = [['reportRefreshDate', 'date', 'no-op'],\r\n",
        "                                ['lastActivityDate', 'date', 'no-op'],\r\n",
        "                                ['deletedDate', 'string', 'no-op'],\r\n",
        "                                ['isDeleted', 'boolean', 'no-op'],\r\n",
        "                                ['isLicensed', 'boolean', 'no-op'], \r\n",
        "                                ['reportPeriod', 'string', 'no-op'],\r\n",
        "                                ['userPrincipalName', 'string', 'hash'],\r\n",
        "                                ['privateChatMessageCount', 'integer', 'no-op'],\r\n",
        "                                ['teamChatMessageCount', 'integer', 'no-op'],\r\n",
        "                                ['meetingsAttendedCount', 'integer', 'no-op'],\r\n",
        "                                ['meetingCount', 'integer', 'no-op'],\r\n",
        "                                ['meetingsOrganizedCount', 'integer', 'no-op'],                        \r\n",
        "                                ['callCount', 'integer', 'no-op'],\r\n",
        "                                ['audioDuration', 'integer', 'no-op'],\r\n",
        "                                ['videoDuration', 'integer', 'no-op'],\r\n",
        "                                ['screenShareDuration', 'integer', 'no-op'],                        \r\n",
        "                                ['scheduledOneTimeMeetingsAttendedCount', 'integer', 'no-op'],\r\n",
        "                                ['scheduledOneTimeMeetingsOrganizedCount', 'integer', 'no-op'],\r\n",
        "                                ['scheduledRecurringMeetingsAttendedCount', 'integer', 'no-op'],\r\n",
        "                                ['scheduledRecurringMeetingsOrganizedCount', 'integer', 'no-op'],\r\n",
        "                                ['adHocMeetingsAttendedCount', 'integer', 'no-op'],\r\n",
        "                                ['adHocMeetingsOrganizedCount', 'integer', 'no-op'],\r\n",
        "                                ['assignedProducts', 'string', 'no-op'],\r\n",
        "                                ['hasOtherAction', 'boolean', 'no-op'],\r\n",
        "                                ['reportYearMonth', 'string', 'partition-by']]\r\n",
        "\r\n",
        "        self.schemas['meetings'] = [['meetingId', 'string', 'no-op'],\r\n",
        "                                ['totalParticipantCount', 'integer', 'no-op'],\r\n",
        "                                ['meetingStartDateTime', 'timestamp', 'no-op'],\r\n",
        "                                ['meetingEndDateTime', 'timestamp', 'no-op'],\r\n",
        "                                ['userEmailAddress', 'string', 'hash'], \r\n",
        "                                ['totalAttendanceInSec', 'integer', 'no-op'],\r\n",
        "                                ['role', 'string', 'no-op'],\r\n",
        "                                ['userId', 'string', 'hash'],\r\n",
        "                                ['userDisplayName', 'string', 'mask'],\r\n",
        "                                ['userTenantId', 'string', 'no-op'],\r\n",
        "                                ['attendanceInterval_joinDateTime', 'timestamp', 'no-op'],\r\n",
        "                                ['attendanceInterval_leaveDateTime', 'timestamp', 'no-op'],                        \r\n",
        "                                ['attendanceInterval_durationInSec', 'integer', 'no-op'],\r\n",
        "                                ['year', 'integer', 'partition-by']]\r\n",
        "    \r\n",
        "    def ingest(self):\r\n",
        "        \"\"\" Processes graphapi data from stage1 into stage2 using structured streaming within the defined functions below. \"\"\"\r\n",
        "        logger.info(\"Processing microsoft_graph data from: \" + self.stage1np)\r\n",
        "\r\n",
        "        items = mssparkutils.fs.ls(self.stage1np)\r\n",
        "        for item in items:\r\n",
        "            if item.name == \"users\":\r\n",
        "                self._process_graphapi_users_stage1_data()\r\n",
        "            elif item.name == \"m365_app_user_detail\":\r\n",
        "                self._process_graphapi_m365_stage1_data()\r\n",
        "            elif item.name == \"teams_activity_user_detail\":\r\n",
        "                self._process_graphapi_teams_stage1_data()\r\n",
        "            elif item.name == \"meeting_attendance_report\":\r\n",
        "                self._process_graphapi_meetings_stage1_data()\r\n",
        "            else:\r\n",
        "                logger.info(\"No defined function for processing this queried data\")\r\n",
        "        \r\n",
        "        logger.info(\"Finished processing graphapi data from stage 1 to stage 2\")\r\n",
        "\r\n",
        "    def _process_graphapi_users_stage1_data(self):\r\n",
        "        \"\"\" Processes users data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
        "        logger.info(\"Processing microsoft_graph users data from: \" + self.stage1np_graphapi_users)\r\n",
        "\r\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "        # read in the raw data, and explode the \"value\" array\r\n",
        "        df = spark.readStream.format('json').load(self.stage1np_graphapi_users + '/*/*.json', header='true')\r\n",
        "        df = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
        "        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
        "        currentDate = datetime.datetime.now()\r\n",
        "        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
        "            # create a new column for partitioning the folder structure\r\n",
        "        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
        "        # use the users_spark_schema for pseudonymization\r\n",
        "        users_spark_schema = oea.to_spark_schema(self.schemas['users'])\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['users'])\r\n",
        "\r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_users + '/_checkpoints_p').partitionBy('ReportYearMonth')\r\n",
        "            query = query.start(self.stage2p + '/users_pseudo')\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_users + '/_checkpoints_np').partitionBy('ReportYearMonth')\r\n",
        "            query2 = query2.start(self.stage2np + '/users_lookup')\r\n",
        "            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "\r\n",
        "    def _process_graphapi_m365_stage1_data(self):\r\n",
        "        \"\"\" Processes m365 data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
        "        logger.info(\"Processing microsoft_graph m365 data from: \" + self.stage1np_graphapi_m365)\r\n",
        "        \r\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "        # read in the raw data, and explode the \"value\" and \"details\" arrays\r\n",
        "        df = spark.readStream.format('json').load(self.stage1np_graphapi_m365 + '/*/*.json', header='true')\r\n",
        "        df = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
        "        df = df.withColumn('reportPeriod', F.explode(F.col('details').reportPeriod)) \\\r\n",
        "                        .withColumn('mobile', F.explode(F.col('details').mobile)) \\\r\n",
        "                        .withColumn('web', F.explode(F.col('details').web)) \\\r\n",
        "                        .withColumn('mac', F.explode(F.col('details').mac)) \\\r\n",
        "                        .withColumn('windows', F.explode(F.col('details').windows)) \\\r\n",
        "                        .withColumn('excel', F.explode(F.col('details').excel)) \\\r\n",
        "                        .withColumn('excelMobile', F.explode(F.col('details').excelMobile)) \\\r\n",
        "                        .withColumn('excelWeb', F.explode(F.col('details').excelWeb)) \\\r\n",
        "                        .withColumn('excelMac', F.explode(F.col('details').excelMac)) \\\r\n",
        "                        .withColumn('excelWindows', F.explode(F.col('details').excelWindows)) \\\r\n",
        "                        .withColumn('oneNote', F.explode(F.col('details').oneNote)) \\\r\n",
        "                        .withColumn('oneNoteMobile', F.explode(F.col('details').oneNoteMobile)) \\\r\n",
        "                        .withColumn('oneNoteWeb', F.explode(F.col('details').oneNoteWeb)) \\\r\n",
        "                        .withColumn('oneNoteMac', F.explode(F.col('details').oneNoteMac)) \\\r\n",
        "                        .withColumn('oneNoteWindows', F.explode(F.col('details').oneNoteWindows)) \\\r\n",
        "                        .withColumn('outlook', F.explode(F.col('details').outlook)) \\\r\n",
        "                        .withColumn('outlookMobile', F.explode(F.col('details').outlookMobile)) \\\r\n",
        "                        .withColumn('outlookWeb', F.explode(F.col('details').outlookWeb)) \\\r\n",
        "                        .withColumn('outlookMac', F.explode(F.col('details').outlookMac)) \\\r\n",
        "                        .withColumn('outlookWindows', F.explode(F.col('details').outlookWindows)) \\\r\n",
        "                        .withColumn('powerPoint', F.explode(F.col('details').powerPoint)) \\\r\n",
        "                        .withColumn('powerPointMobile', F.explode(F.col('details').powerPointMobile)) \\\r\n",
        "                        .withColumn('powerPointWeb', F.explode(F.col('details').powerPointWeb)) \\\r\n",
        "                        .withColumn('powerPointMac', F.explode(F.col('details').powerPointMac)) \\\r\n",
        "                        .withColumn('powerPointWindows', F.explode(F.col('details').powerPointWindows)) \\\r\n",
        "                        .withColumn('teams', F.explode(F.col('details').teams)) \\\r\n",
        "                        .withColumn('teamsMobile', F.explode(F.col('details').teamsMobile)) \\\r\n",
        "                        .withColumn('teamsWeb', F.explode(F.col('details').teamsWeb)) \\\r\n",
        "                        .withColumn('teamsMac', F.explode(F.col('details').teamsMac)) \\\r\n",
        "                        .withColumn('teamsWindows', F.explode(F.col('details').teamsWindows)) \\\r\n",
        "                        .withColumn('word', F.explode(F.col('details').word)) \\\r\n",
        "                        .withColumn('wordMobile', F.explode(F.col('details').wordMobile)) \\\r\n",
        "                        .withColumn('wordWeb', F.explode(F.col('details').wordWeb)) \\\r\n",
        "                        .withColumn('wordMac', F.explode(F.col('details').wordMac)) \\\r\n",
        "                        .withColumn('wordWindows', F.explode(F.col('details').wordWindows)) \\\r\n",
        "                        .drop('details')\r\n",
        "        # change columns with dates to be of date types\r\n",
        "        df.select(F.col('reportRefreshDate'), F.to_date(F.col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
        "        df.select(F.col('lastActivityDate'), F.to_date(F.col('lastActivityDate'), 'yyyy-MM-dd'))\r\n",
        "        df.select(F.col('lastActivationDate'), F.to_date(F.col('lastActivationDate'), 'yyyy-MM-dd'))\r\n",
        "        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
        "        currentDate = datetime.datetime.now()\r\n",
        "        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
        "            # create a new column for partitioning the folder structure\r\n",
        "        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
        "        # use the m365_spark_schema for pseudonymization\r\n",
        "        m365_spark_schema = oea.to_spark_schema(self.schemas['m365'])\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['m365'])\r\n",
        "\r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_m365 + '/_checkpoints_p').partitionBy('ReportYearMonth')\r\n",
        "            query = query.start(self.stage2p + '/m365_app_user_detail_pseudo')\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_m365 + '/_checkpoints_np').partitionBy('ReportYearMonth')\r\n",
        "            query2 = query2.start(self.stage2np + '/m365_app_user_detail_lookup')\r\n",
        "            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs. \r\n",
        "\r\n",
        "    def _process_graphapi_teams_stage1_data(self):\r\n",
        "        \"\"\" Processes teams data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
        "        logger.info(\"Processing microsoft_graph teams data from: \" + self.stage1np_graphapi_teams)\r\n",
        "\r\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "        # read in the raw data, and explode the \"value\" and \"assignedProducts\" arrays \r\n",
        "        df = spark.readStream.format('json').load(self.stage1np_graphapi_teams + '/*/*.json', header='true')\r\n",
        "        df = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
        "        df = df.withColumn('assignedProducts', F.explode(F.col('assignedProducts')))\r\n",
        "            # convert duration to seconds only \r\n",
        "            # NOTE: The duration expression may have changed and this will need to be modified to accommodate any new duration formatting\r\n",
        "        df = df.withColumn(\r\n",
        "            'screenShareDuration', \r\n",
        "            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
        "            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
        "            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
        "            ).withColumn(\r\n",
        "            'videoDuration', \r\n",
        "            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
        "            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
        "            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
        "            ).withColumn(\r\n",
        "            'audioDuration', \r\n",
        "            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
        "            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
        "            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
        "            )\r\n",
        "        # change columns with dates to be of date types\r\n",
        "        df.select(F.col('reportRefreshDate'), F.to_date(F.col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
        "        df.select(F.col('lastActivityDate'), F.to_date(F.col('lastActivityDate'), 'yyyy-MM-dd'))\r\n",
        "        # uncomment this code when using actual data, since this will be null in the test data\r\n",
        "        #df.select(F.col('deletedDate'), F.to_date(F.col('deletedDate'), 'yyyy-MM-dd'))\r\n",
        "        # grab the current date for partitioning the data later (in stage 2 folders)\r\n",
        "        currentDate = datetime.datetime.now()\r\n",
        "        currentYearMonth = currentDate.strftime('%Y-%m')\r\n",
        "            # create a new column for partitioning the folder structure\r\n",
        "        df = df.withColumn('ReportYearMonth', F.lit(currentYearMonth))\r\n",
        "        # use the teams_spark_schema for pseudonymization\r\n",
        "        teams_spark_schema = oea.to_spark_schema(self.schemas['teams'])\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['teams'])\r\n",
        "\r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_teams + '/_checkpoints_p').partitionBy('ReportYearMonth')\r\n",
        "            query = query.start(self.stage2p + '/teams_activity_user_detail_pseudo')\r\n",
        "            query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_teams + '/_checkpoints_np').partitionBy('ReportYearMonth')\r\n",
        "            query2 = query2.start(self.stage2np + '/teams_activity_user_detail_lookup')\r\n",
        "            query2.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
        "\r\n",
        "    def _process_graphapi_meetings_stage1_data(self):\r\n",
        "        \"\"\" Processes meeting attendance report data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
        "        logger.info(\"Processing microsoft_graph meetings data from: \" + self.stage1np_graphapi_meetings)\r\n",
        "\r\n",
        "        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "        # read in the raw data, drop irrelevant data, and rename columns.\r\n",
        "        # then, explode the \"attendanceRecords\", \"identity\", and \"attendanceIntervals\" arrays\r\n",
        "        # NOTE: the multiLine option is currently set as true for the test data, if this does not match the test data, remove this parameter\r\n",
        "        df = spark.readStream.format('json').load(self.stage1np_graphapi_meetings + '/*/*.json', header='true', multiLine='true')\r\n",
        "        #df = df.drop('@odata.context')\r\n",
        "        # flatten the original nested JSON file format\r\n",
        "        dfFlat = df.select(\r\n",
        "            \"id\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\",\r\n",
        "            F.explode(\"attendanceRecords\").alias(\"attendanceRecordsExplode\")\r\n",
        "        ).select(\"id\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \r\n",
        "                    \"attendanceRecordsExplode.*\")\r\n",
        "\r\n",
        "        dfFlat = dfFlat.withColumnRenamed(\"id\",\"meetingId\")\r\n",
        "\r\n",
        "        dfFlat = dfFlat.select(\r\n",
        "            \"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \r\n",
        "            \"totalAttendanceInSeconds\", \"role\", \"emailAddress\", \"attendanceIntervals\",\r\n",
        "            F.explode(F.array(\"identity\")).alias(\"identityExplode\")\r\n",
        "        ).select(\"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \r\n",
        "            \"totalAttendanceInSeconds\", \"role\", \"emailAddress\",\"attendanceIntervals\",\r\n",
        "                \"identityExplode.*\")\r\n",
        "\r\n",
        "        dfFlat = dfFlat.select(\r\n",
        "            \"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \r\n",
        "            \"totalAttendanceInSeconds\", \"role\", \"emailAddress\",\r\n",
        "            \"displayName\", \"id\", \"tenantId\",\r\n",
        "            F.explode(\"attendanceIntervals\").alias(\"attendanceIntervalsExplode\")\r\n",
        "        ).select(\"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \r\n",
        "            \"totalAttendanceInSeconds\", \"role\", \"emailAddress\",\r\n",
        "            \"attendanceIntervalsExplode.*\",\r\n",
        "            \"displayName\", \"id\", \"tenantId\")\r\n",
        "\r\n",
        "        # clean up the column names and data types\r\n",
        "        dfFlat = dfFlat.withColumnRenamed(\"id\", \"userId\").withColumnRenamed(\"displayName\", \"userDisplayName\").withColumnRenamed(\"emailAddress\", \"userEmailAddress\") \\\r\n",
        "                .withColumnRenamed(\"totalAttendanceInSeconds\", \"totalAttendanceInSec\").withColumnRenamed(\"tenantId\", \"userTenantId\") \\\r\n",
        "                .withColumnRenamed(\"joinDateTime\", \"attendanceInterval_joinDateTime\").withColumnRenamed(\"leaveDateTime\", \"attendanceInterval_leaveDateTime\").withColumnRenamed(\"durationInSeconds\", \"attendanceInterval_durationInSec\")\r\n",
        "        dfFlat = dfFlat.withColumn('meetingStartDateTime', F.to_timestamp(F.col('meetingStartDateTime'))) \\\r\n",
        "                .withColumn('meetingEndDateTime', F.to_timestamp(F.col('meetingEndDateTime'))) \\\r\n",
        "                .withColumn('attendanceInterval_joinDateTime', F.to_timestamp(F.col('attendanceInterval_joinDateTime'))) \\\r\n",
        "                .withColumn('attendanceInterval_leaveDateTime', F.to_timestamp(F.col('attendanceInterval_leaveDateTime')))\r\n",
        "        \r\n",
        "        # use the meetingEndDateTime column for partitioning the files and pseudonymize\r\n",
        "        df = dfFlat.withColumn('year', F.year(F.col('meetingEndDateTime')))\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, self.schemas['meetings'])\r\n",
        "\r\n",
        "        if len(df_pseudo.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2p')\r\n",
        "        else:\r\n",
        "            query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_meetings + '/_checkpoints_p').partitionBy('year')\r\n",
        "            query = query.start(self.stage2p + '/meeting_attendance_report_pseudo')\r\n",
        "            query.awaitTermination()\r\n",
        "        \r\n",
        "        if len(df_lookup.columns) == 0:\r\n",
        "            logger.info('No data to be written to stage2np')\r\n",
        "        else:\r\n",
        "            query2 = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_meetings + '/_checkpoints_np').partitionBy('year')\r\n",
        "            query2 = query2.start(self.stage2np + '/meeting_attendance_report_lookup')\r\n",
        "            query2.awaitTermination()\r\n",
        "        "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}