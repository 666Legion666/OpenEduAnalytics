{
  "metadata": {
    "saveOutput": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contoso SIS\n",
        "This notebook is for processing data for the fictional Contoso ISD.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "storage_account = 'steduanalyticstestx3'\n",
        "use_test_env = True"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "if use_test_env:\n",
        "    stage1 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1'\n",
        "    stage2 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2'\n",
        "    stage3 = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage3'\n",
        "else:\n",
        "    stage1 = 'abfss://stage1@' + storage_account + '.dfs.core.windows.net'\n",
        "    stage2 = 'abfss://stage2@' + storage_account + '.dfs.core.windows.net'\n",
        "    stage3 = 'abfss://stage3@' + storage_account + '.dfs.core.windows.net'"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import sha2, lit\n",
        "\n",
        "# Process studentsectionmark and studentattendance\n",
        "df = spark.read.csv(stage1 + '/contoso_sis/studentsectionmark.csv', header='true', inferSchema='true')\n",
        "df = df.withColumn('id',df.id.cast('string')).withColumn('student_id',df.student_id.cast('string'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/contoso_sis/studentsectionmark')\n",
        "\n",
        "df = spark.read.csv(stage1 + '/contoso_sis/studentattendance.csv', header='true', inferSchema='true')\n",
        "df = df.withColumn('id',df.id.cast('string')).withColumn('student_id',df.student_id.cast('string'))\n",
        "df.write.format('parquet').mode('overwrite').save(stage2 + '/contoso_sis/studentattendance')\n",
        "\n",
        "# Anonymize data and load into stage3\n",
        "df = spark.read.format('parquet').load(stage2 + '/contoso_sis/studentsectionmark')\n",
        "df = df.withColumn('id', sha2(df.id, 256)).withColumn('student_id',sha2(df.student_id, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/contoso_sis/studentsectionmark')\n",
        "\n",
        "df = spark.read.format('parquet').load(stage2 + '/contoso_sis/studentattendance')\n",
        "df = df.withColumn('id', sha2(df.id, 256)).withColumn('student_id',sha2(df.student_id, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(stage3 + '/contoso_sis/studentattendance')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql('CREATE DATABASE IF NOT EXISTS ' + db_name)\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".studentsectionmark using PARQUET location '\" + source_path + \"/studentsectionmark'\")\n",
        "    spark.sql(\"create table if not exists \" + db_name + \".studentattendance using PARQUET location '\" + source_path + \"/studentattendance'\")\n",
        "\n",
        "# Drop all tables in a db, then drop the db\n",
        "def drop_db(db_name):\n",
        "    df = spark.sql('SHOW TABLES FROM ' + db_name)\n",
        "    for row in df.rdd.collect():\n",
        "        spark.sql('DROP TABLE IF EXISTS ' + db_name + '.' + row['tableName'])\n",
        "    spark.sql('DROP DATABASE ' + db_name )\n",
        "\n",
        "#drop_db('s2_contoso_sis')\n",
        "create_spark_db('s2_contoso_sis', stage2 + '/contoso_sis')\n",
        "#drop_db('s3_contoso_sis')\n",
        "create_spark_db('s3_contoso_sis', stage3 + '/contoso_sis')"
      ],
      "attachments": {}
    }
  ]
}