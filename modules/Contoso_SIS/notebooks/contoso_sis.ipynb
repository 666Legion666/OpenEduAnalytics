{
  "metadata": {
    "saveOutput": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## anon_edu_dl\n",
        "This notebook is for creating a consolidated view over the data from each of the source systems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "storage_account = 'steduanalyticsx5'\n",
        "\n",
        "stage1 = f'abfss://test-data@{storage_account}.dfs.core.windows.net/stage1'\n",
        "#stage1 = f'abfss://stage1@{storage_account}.dfs.core.windows.net'\n",
        "stage2 = f'abfss://stage2@{storage_account}.dfs.core.windows.net'\n",
        "stage3 = f'abfss://stage3@{storage_account}.dfs.core.windows.net'"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Process sectionmark.csv data (from stage1 into stage2 and stage3)\n",
        "# Convert id values to use the Person.Id and Section.Id values set in the Education Data Platform.\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage1}/contoso_sis/studentsectionmark'), 'SectionMark')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Person'), 'Person')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Section'), 'Section')\n",
        "\n",
        "df = spark.sql(\"select sm.id Id, p.Id PersonId, s.Id SectionId, cast(sm.numeric_grade_earned as int) NumericGrade, \\\n",
        "sm.alpha_grade_earned AlphaGrade, sm.is_final_grade IsFinalGrade, cast(sm.credits_attempted as int) CreditsAttempted, cast(sm.credits_earned as int) CreditsEarned, \\\n",
        "sm.grad_credit_type GraduationCreditType, sm.id ExternalId, CURRENT_TIMESTAMP CreateDate, CURRENT_TIMESTAMP LastModifiedDate, true IsActive \\\n",
        "from SectionMark sm, Person p, Section s \\\n",
        "where sm.student_id = p.ExternalId \\\n",
        "and sm.section_id = s.ExternalId\")\n",
        "\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage2}/contoso_sis/SectionMark')\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage2}/contoso_sis/SectionMark2')\n",
        "\n",
        "# Add SectionMark data to stage3 (anonymized parquet lake)\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/contoso_sis/SectionMark')\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/contoso_sis/SectionMark2')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Process studentattendance.csv data (from stage1 into stage2 and stage3)\n",
        "# Convert id values to use the Person.Id, Org.Id and Section.Id values set in the Education Data Platform.\n",
        "from pyspark.sql.functions import sha2, lit\n",
        "\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage1}/contoso_sis/studentattendance'), 'Attendance')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Org'), 'Org')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Person'), 'Person')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Section'), 'Section')\n",
        "df = spark.sql(\"select att.id Id, p.Id PersonId, att.school_year SchoolYear, o.Id OrgId, to_date(att.attendance_date,'MM/dd/yyyy') AttendanceDate, \\\n",
        "att.all_day AllDay, att.Period Period, s.Id SectionId, att.AttendanceCode AttendanceCode, att.PresenceFlag PresenceFlag, \\\n",
        "att.attendance_status AttendanceStatus, att.attendance_type AttendanceType, att.attendance_sequence AttendanceSequence \\\n",
        "from Attendance att, Org o, Person p, Section s \\\n",
        "where att.student_id = p.ExternalId \\\n",
        "and att.school_id = o.ExternalId \\\n",
        "and att.section_id = s.ExternalId\")\n",
        "\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage2}/contoso_sis/Attendance')\n",
        "\n",
        "# Add Attendance data to stage3 (anonymized parquet lake)\n",
        "df = df.withColumn('PersonId', sha2(df.PersonId, 256))\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/contoso_sis/Attendance')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+--------------------+--------------------+--------+--------------------+----------+-------------------+-------------------+--------+--------------------+--------------------+\n|                  Id|                Name|    Code|         Description|ExternalId|         CreateDate|   LastModifiedDate|IsActive|          CalendarId|          Department|\n+--------------------+--------------------+--------+--------------------+----------+-------------------+-------------------+--------+--------------------+--------------------+\n|7DB1CB81-C0A2-4A2...|ATTENDANCE - PRE-...|  EATT-P|ATTENDANCE - PRE-...|      3880|2020-05-21 06:05:46|2020-11-05 10:50:24|    true|3A5E8662-9DF9-4C6...|      Not Applicable|\n|127522A9-31D2-4A8...|ATTENDANCE - KIND...|  EATT-K|ATTENDANCE - KIND...|      3884|2020-05-21 06:05:46|2020-11-05 10:50:24|    true|3A5E8662-9DF9-4C6...|      Not Applicable|\n|FC2A5E68-A100-47D...|ATTENDANCE - GRADE 1|  EATT-1|ATTENDANCE - GRADE 1|      3886|2020-05-21 06:05:46|2020-11-05 10:50:24|    true|3A5E8662-9DF9-4C6...|      Not Applicable|\n|070611E6-2628-45C...|ATTENDANCE - GRADE 2|  EATT-2|ATTENDANCE - GRADE 2|      3888|2020-05-21 06:05:46|2020-11-05 10:50:24|    true|3A5E8662-9DF9-4C6...|      Not Applicable|\n|E260018D-D233-466...|ATTENDANCE - GRADE 3|  EATT-3|ATTENDANCE - GRADE 3|      3890|2020-05-21 06:05:46|2020-11-05 10:50:24|    true|3A5E8662-9DF9-4C6...|      Not Applicable|\n|1EE574E3-4F97-4F4...|ATTENDANCE - GRADE 4|  EATT-4|ATTENDANCE - GRADE 4|      3892|2020-05-21 06:05:46|2020-11-05 10:50:24|    true|3A5E8662-9DF9-4C6...|      Not Applicable|\n|8FDD90D9-6532-4AB...|ATTENDANCE - GRADE 5|  EATT-5|ATTENDANCE - GRADE 5|      3894|2020-05-21 06:05:46|2020-11-05 10:50:24|    true|3A5E8662-9DF9-4C6...|      Not Applicable|\n|1EAE25DF-3387-47A...|ATTENDANCE - GRADE 6|  EATT-6|ATTENDANCE - GRADE 6|      3896|2020-05-21 06:05:46|2020-11-05 10:50:24|    true|3A5E8662-9DF9-4C6...|      Not Applicable|\n|939102D6-66E0-4F2...|        AP Engl Lang|1000ELAB|AP English Langua...|      4148|2020-05-21 06:05:46|2020-11-05 10:50:24|    true|3A5E8662-9DF9-4C6...|English Language ...|\n|84C59B23-98ED-47C...|           English I|1020ELAB|           English I|      4152|2020-05-21 06:05:46|2020-11-05 10:50:24|    true|3A5E8662-9DF9-4C6...|English Language ...|\n+--------------------+--------------------+--------+--------------------+----------+-------------------+-------------------+--------+--------------------+--------------------+\nonly showing top 10 rows"
          },
          "execution_count": 10,
          "metadata": {}
        }
      ],
      "metadata": {
        "outputCollapsed": true
      },
      "source": [
        "# Add 'Department' column to Course in contoso_sis\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/m365/Course'), 'Course')\n",
        "sqlContext.registerDataFrameAsTable(spark.read.format('parquet').load(f'{stage2}/contoso_sis/course'), 'dsCourse')\n",
        "\n",
        "df = spark.sql(\"select c1.Id, c1.Name, c1.Code, c1.Description, c1.ExternalId, c1.CreateDate, \\\n",
        "c1.LastModifiedDate, c1.IsActive, c1.CalendarId, c2.department Department \\\n",
        "from Course c1, dsCourse c2 \\\n",
        "where c1.ExternalId = c2.id\")\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage2}/contoso_sis/Course')\n",
        "df.write.format('parquet').mode('overwrite').save(f'{stage3}/contoso_sis/Course')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\n",
        "def create_spark_db(db_name, source_path):\n",
        "    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\n",
        "    spark.sql(f\"create table if not exists {db_name}.Activity using PARQUET location '{source_path}/m365/Activity'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Calendar using PARQUET location '{source_path}/m365/Calendar'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Org using PARQUET location '{source_path}/m365/Org'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Person using PARQUET location '{source_path}/m365/Person'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.PersonIdentifier using PARQUET location '{source_path}/m365/PersonIdentifier'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.RefDefinition using PARQUET location '{source_path}/m365/RefDefinition'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Section using PARQUET location '{source_path}/m365/Section'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Session using PARQUET location '{source_path}/m365/Session'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StaffOrgAffiliation using PARQUET location '{source_path}/m365/StaffOrgAffiliation'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StaffSectionMembership using PARQUET location '{source_path}/m365/StaffSectionMembership'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StudentOrgAffiliation using PARQUET location '{source_path}/m365/StudentOrgAffiliation'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.StudentSectionMembership using PARQUET location '{source_path}/m365/StudentSectionMembership'\")\n",
        "\n",
        "    spark.sql(f\"create table if not exists {db_name}.Course using PARQUET location '{source_path}/edu_dl/Course'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.Attendance using PARQUET location '{source_path}/edu_dl/Attendance'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.SectionMark using PARQUET location '{source_path}/edu_dl/SectionMark'\")\n",
        "    spark.sql(f\"create table if not exists {db_name}.SectionMark2 using PARQUET location '{source_path}/edu_dl/SectionMark2'\")\n",
        "\n",
        "create_spark_db('edu_dl', stage2)\n",
        "create_spark_db('anon_edu_dl', stage3)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Drop all tables in a db, then drop the db\n",
        "def drop_db(db_name):\n",
        "    df = spark.sql('SHOW TABLES FROM ' + db_name)\n",
        "    for row in df.rdd.collect():\n",
        "        spark.sql(f\"DROP TABLE IF EXISTS {db_name}.{row['tableName']}\")\n",
        "    spark.sql(f\"DROP DATABASE {db_name}\")\n",
        "\n",
        "drop_db('db_name')"
      ],
      "attachments": {}
    }
  ]
}