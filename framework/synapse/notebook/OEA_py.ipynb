{
	"name": "OEA_py",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark3p2sm",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fd3bb07f-dd10-414b-aea8-d6ad344ac2a1"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7b9a4896-4541-483f-bdc7-d8f4ec6be3ee/resourceGroups/rg-oea-cisd31101e/providers/Microsoft.Synapse/workspaces/syn-oea-cisd31101e/bigDataPools/spark3p2sm",
				"name": "spark3p2sm",
				"type": "Spark",
				"endpoint": "https://syn-oea-cisd31101e.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark3p2sm",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"####### OEA configuration #############\r\n",
					"oea_storage_account = 'yourstorageaccount'\r\n",
					"oea_keyvault = 'yourkeyvault'\r\n",
					"oea_timezone = 'US/Eastern'\r\n",
					"#######################################\r\n",
					"\r\n",
					"from delta.tables import DeltaTable\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType, BooleanType, ShortType, DateType\r\n",
					"from pyspark.sql import functions as F\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.utils import AnalysisException\r\n",
					"import logging\r\n",
					"import pandas as pd\r\n",
					"import sys\r\n",
					"import re\r\n",
					"import json\r\n",
					"import datetime\r\n",
					"import pytz\r\n",
					"import random\r\n",
					"import io\r\n",
					"import urllib.request\r\n",
					"\r\n",
					"logger = logging.getLogger('OEA')\r\n",
					"\r\n",
					"class OEA:\r\n",
					"    \"\"\" OEA (Open Education Analytics) framework simplifies the process of working with large data sets within the context of a lakehouse architecture.\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    def __init__(self, workspace='dev', logging_level=logging.INFO, storage_account=None, keyvault=None, timezone=None):\r\n",
					"        self.keyvault_linked_service = 'LS_KeyVault'\r\n",
					"        self.salt_secret_name = 'oeaSalt'\r\n",
					"        self.salt = None\r\n",
					"        self.workspace = workspace\r\n",
					"        self.storage_account = oea_storage_account\r\n",
					"        self.keyvault = oea_keyvault\r\n",
					"        self.timezone = oea_timezone\r\n",
					"\r\n",
					"        # pull in override values if any were passed in\r\n",
					"        if workspace: self.workspace = workspace\r\n",
					"        if storage_account: self.storage_account = storage_account\r\n",
					"        if keyvault: self.keyvault = keyvault \r\n",
					"        if timezone: self.timezone = timezone\r\n",
					"        if logging_level: self.logging_level = logging_level    \r\n",
					"\r\n",
					"        self._initialize_logger(logging_level)\r\n",
					"        self.set_workspace(self.workspace)\r\n",
					"        spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # more info here: https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/optimize-write-for-apache-spark\r\n",
					"        logger.info(\"OEA initialized.\")\r\n",
					"\r\n",
					"    def _initialize_logger(self, logging_level):\r\n",
					"        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\r\n",
					"        for handler in logging.getLogger().handlers:\r\n",
					"            handler.setFormatter(formatter)           \r\n",
					"        # Customize log level for all loggers\r\n",
					"        logging.getLogger().setLevel(logging_level)        \r\n",
					"\r\n",
					"    def _get_secret(self, secret_name):\r\n",
					"        \"\"\" Retrieves the specified secret from the keyvault.\r\n",
					"            This method assumes that the keyvault linked service has been setup and is accessible.\r\n",
					"        \"\"\"\r\n",
					"        sc = SparkSession.builder.getOrCreate()\r\n",
					"        token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
					"        value = token_library.getSecret(self.keyvault, secret_name, self.keyvault_linked_service)        \r\n",
					"        return value\r\n",
					"\r\n",
					"    def _get_salt(self):\r\n",
					"        if not self.salt:\r\n",
					"            self.salt = self._get_secret(self.salt_secret_name)\r\n",
					"        return self.salt\r\n",
					"\r\n",
					"    def set_workspace(self, workspace_name):\r\n",
					"        \"\"\" Allows you to use OEA against your workspace\r\n",
					"            (eg, you specify Jon as workspace_name, then instead of reading in from stage1 OEA will use workspace/Jon/stage1\r\n",
					"        \"\"\"\r\n",
					"        \r\n",
					"        if workspace_name == 'prod' or workspace_name == 'production':\r\n",
					"            self.workspace = 'prod'\r\n",
					"            self.stage1 = 'abfss://stage1@' + self.storage_account + '.dfs.core.windows.net'\r\n",
					"            self.stage2 = 'abfss://stage2@' + self.storage_account + '.dfs.core.windows.net'\r\n",
					"            self.stage3 = 'abfss://stage3@' + self.storage_account + '.dfs.core.windows.net'\r\n",
					"        elif workspace_name == 'dev' or workspace_name == 'development':\r\n",
					"            self.workspace = 'dev'\r\n",
					"            self.stage1 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage1'\r\n",
					"            self.stage2 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage2'\r\n",
					"            self.stage3 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/dev/stage3'\r\n",
					"        else:\r\n",
					"            self.workspace = workspace_name\r\n",
					"            self.stage1 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage1'\r\n",
					"            self.stage2 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage2'\r\n",
					"            self.stage3 = f'abfss://oea@{self.storage_account}.dfs.core.windows.net/sandboxes/{workspace_name}/stage3'\r\n",
					"        logger.info(f'Now using workspace: {self.workspace}')\r\n",
					"\r\n",
					"    def to_url(self, path):\r\n",
					"        \"\"\" Converts the given path into a valid url.\r\n",
					"            eg, convert_path('stage1/contoso_sis/student') # returns abfss://stage1@storageaccount.dfs.core.windows.net/contoso_sis/student\r\n",
					"            [Note that if \"use_sandbox\" has been invoked, the url returned will be something like abfss://dev@storageaccount.dfs.core.windows.net/sandbox1/stage1/contoso_sis/student]\r\n",
					"        \"\"\"\r\n",
					"        if not path or path == '': raise ValueError('Specified path cannot be empty.')\r\n",
					"        if path.startswith('abfss://'): return path # if a url is given, just return that same url (allows to_url to be invoked just in case translation may be needed)\r\n",
					"        path_args = path.split('/')\r\n",
					"        stage = path_args.pop(0)\r\n",
					"        if stage == 'stage1': stage = self.stage1\r\n",
					"        elif stage == 'stage2': stage = self.stage2\r\n",
					"        elif stage == 'stage3': stage = self.stage3\r\n",
					"        else: raise ValueError(\"Path must begin with either 'stage1', 'stage2', or 'stage3'\")\r\n",
					"        url = f\"{stage}/{'/'.join(path_args)}\"\r\n",
					"        logger.debug(f'to_url: {url}')\r\n",
					"        return url\r\n",
					"\r\n",
					"    def parse_path(self, path):\r\n",
					"        \"\"\" Parses a path that looks like one of the following:\r\n",
					"                stage1/Transactional/ms_insights/v0.1\r\n",
					"                stage1/Transactional/ms_insights/v0.1/students\r\n",
					"            (the path must either be the path to a specific entity, or the path to the parent folder containing entities)\r\n",
					"            and returns a dictionary like one of the following:\r\n",
					"                {'stage': 'stage1', 'stage_num': '1', 'category': 'Transactional', 'source_system': 'contoso_sis', 'entity': None, 'entity_list': ['studentattendance'], 'entity_path': None, 'entity_parent_path': 'stage1/Transactional/contoso_sis/v0.1'}\r\n",
					"                {'stage': 'stage1', 'stage_num': '1', 'category': 'Transactional', 'source_system': 'contoso_sis', 'entity': 'studentattendance', 'entity_list': None, 'entity_path': 'stage1/Transactional/contoso_sis/v0.1/studentattendance', 'entity_parent_path': 'stage1/Transactional/contoso_sis/v0.1'}\r\n",
					"\r\n",
					"            This method assumes the standard OEA data lake, in which paths have this structure: <stage number>/<category>/<source system>/<optional version and partitioning>/<entity>/<either batch_data folder or _delta_log>\r\n",
					"        \"\"\"\r\n",
					"        if type(path) is dict: return path # this means the path was already parsed\r\n",
					"        \r\n",
					"        ar = path.split('/')\r\n",
					"        path_dict = {'stage':ar[0], 'stage_num':ar[0][-1], 'category':ar[1], 'source_system':ar[2], 'entity':None, 'entity_list':None, 'entity_path':None, 'entity_parent_path':None}\r\n",
					"\r\n",
					"        folders = self.get_folders(self.to_url(path))\r\n",
					"\r\n",
					"        # Identify an entity folder by the presence of the \"_delta_log\" folder in stage2 and stage3\r\n",
					"        if (path_dict['stage_num'] == '1' and ('additive_batch_data' in folders or 'delta_batch_data' in folders or 'snapshot_batch_data' in folders)) or ((path_dict['stage_num'] == '2' or path_dict['stage_num'] == '3') and '_delta_log' in folders):\r\n",
					"            path_dict['entity'] = ar[-1]\r\n",
					"            path_dict['entity_path'] = path\r\n",
					"            path_dict['entity_parent_path'] = '/'.join(ar[0:-1]) # eg, stage1/Transactional/contoso_sis/v0.1\r\n",
					"        else:\r\n",
					"            path_dict['entity_list'] = folders\r\n",
					"            path_dict['entity_parent_path'] = path\r\n",
					"\r\n",
					"        if path_dict['stage'] == 'stage2':\r\n",
					"            abbrev = path_dict['category'][0].lower() # either 'i' for Ingested or 'r' for Refined\r\n",
					"            path_dict['sdb_name'] = f'sdb_{self.workspace}_s{path_dict[\"stage_num\"]}{abbrev}_{path_dict[\"source_system\"].lower()}' # name of the sql db for this source (use lower case to match the naming for lake db)\r\n",
					"            path_dict['ldb_name'] = f'ldb_{self.workspace}_s{path_dict[\"stage_num\"]}{abbrev}_{path_dict[\"source_system\"].lower()}' # name of the lake db for this source (spark will automatically lower case the name of the db, but we're doing it here to be explicit)\r\n",
					"        else:\r\n",
					"            path_dict['sdb_name'] = f'sdb_{self.workspace}_s{path_dict[\"stage_num\"]}_{path_dict[\"source_system\"].lower()}' # name of the sql db for this source (use lower case to match the naming for lake db)\r\n",
					"            path_dict['ldb_name'] = f'ldb_{self.workspace}_s{path_dict[\"stage_num\"]}_{path_dict[\"source_system\"].lower()}' # name of the lake db for this source (spark will automatically lower case the name of the db, but we're doing it here to be explicit)\r\n",
					"        \r\n",
					"        path_dict['between_path'] = '/'.join(path_dict['entity_parent_path'].split('/')[2:]) # strip off the first 2 args in the entity parent path (eg, strip off stage1/Transactional which leaves contoso_sis/v0.1)\r\n",
					"\r\n",
					"        m = re.match(r'.*\\/(v[^\\/]+).*', path_dict['between_path'])\r\n",
					"        if m:\r\n",
					"            path_dict['version'] = m.group(1)\r\n",
					"            # Append the version number to the db names. First replace the '.' char with a 'p' if necessary (because a '.' is not allowed in the db name)\r\n",
					"            safe_version = re.sub('\\.', 'p', path_dict[\"version\"])\r\n",
					"            path_dict['sdb_name'] = f'{path_dict[\"sdb_name\"]}_{safe_version}'\r\n",
					"            path_dict['ldb_name'] = f'{path_dict[\"ldb_name\"]}_{safe_version}'\r\n",
					"        else:\r\n",
					"            path_dict['version'] = None\r\n",
					"\r\n",
					"        return path_dict\r\n",
					"\r\n",
					"    def rm_if_exists(self, path, recursive_remove=True):\r\n",
					"        \"\"\" Remove a folder if it exists (defaults to use of recursive removal). \"\"\"\r\n",
					"        try:\r\n",
					"            mssparkutils.fs.rm(self.to_url(path), recursive_remove)\r\n",
					"        except Exception as e:\r\n",
					"            pass\r\n",
					"\r\n",
					"    def ls(self, path):\r\n",
					"        \"\"\" List the contents of the given path. \"\"\"\r\n",
					"        url = self.to_url(path)\r\n",
					"        folders = []\r\n",
					"        files = []\r\n",
					"        try:\r\n",
					"            items = mssparkutils.fs.ls(url)\r\n",
					"            for item in items:\r\n",
					"                if item.isFile:\r\n",
					"                    files.append(item.name)\r\n",
					"                elif item.isDir:\r\n",
					"                    folders.append(item.name)\r\n",
					"        except Exception as e:\r\n",
					"            logger.warning(\"[OEA] Could not peform ls on specified path: \" + path + \"\\nThis may be because the path does not exist.\")\r\n",
					"        return (folders, files)\r\n",
					"\r\n",
					"    def path_exists(self, path):\r\n",
					"        \"\"\" Returns true if path exists, false if it doesn't (no exception will be thrown). \r\n",
					"            eg, path_exists('stage1/mytest/v1.0')\r\n",
					"        \"\"\"\r\n",
					"        try:\r\n",
					"            items = mssparkutils.fs.ls(self.to_url(path))\r\n",
					"        except Exception as e:\r\n",
					"            # This Exception comes as a generic Py4JJavaError that occurs when the path specified is not found.\r\n",
					"            return False\r\n",
					"        return True\r\n",
					"\r\n",
					"    def get_stage_num(self, path):\r\n",
					"        m = re.match(r'.*stage(\\d)/.*', path)\r\n",
					"        if m:\r\n",
					"            return m.group(1)\r\n",
					"        else:\r\n",
					"            raise ValueError(\"Path must begin with either 'stage1', 'stage2', or 'stage3'\")\r\n",
					"\r\n",
					"    def get_folders(self, path):\r\n",
					"        \"\"\" Return the list of folders found in the given path. \"\"\"\r\n",
					"        dirs = []\r\n",
					"        try:\r\n",
					"            items = mssparkutils.fs.ls(self.to_url(path))\r\n",
					"            for item in items:\r\n",
					"                #print(item.name, item.isDir, item.isFile, item.path, item.size)\r\n",
					"                if item.isDir:\r\n",
					"                    dirs.append(item.name)\r\n",
					"        except Exception as e:\r\n",
					"            logger.warning(\"[OEA] Could not get list of folders in specified path: \" + path + \"\\nThis may be because the path does not exist.\")\r\n",
					"        return dirs\r\n",
					"\r\n",
					"    def get_latest_folder(self, path):\r\n",
					"        \"\"\" Gets the last folder listed in the given path. \"\"\"\r\n",
					"        folders = self.get_folders(path)\r\n",
					"        if len(folders) > 0: return folders[-1]\r\n",
					"        else: return None\r\n",
					"\r\n",
					"    def contains_batch_folder(self, path):\r\n",
					"        for name in self.get_folders(self.to_url(path)):\r\n",
					"            if name == 'additive_batch_data' or name == 'snapshot_batch_data' or name == 'delta_batch_data':\r\n",
					"                return True\r\n",
					"        return False\r\n",
					"\r\n",
					"    def get_batch_info(self, source_path):\r\n",
					"        \"\"\" Given a source data path, returns a tuple with the batch type (based on the name of the folder) and file type (based on a file extension) \r\n",
					"            eg, get_batch_info('stage1/Transactional/sis/v1.0/students') # returns ('snapshot', 'csv')\r\n",
					"        \"\"\"\r\n",
					"        url = self.to_url(source_path)\r\n",
					"        source_folder_name = self.get_latest_folder(url) #expects to find one of: additivie_batch_data, snapshot_batch_data, delta_batch_data\r\n",
					"        batch_type = source_folder_name.split('_')[0]\r\n",
					"\r\n",
					"        rundate_dir = self.get_latest_folder(f'{url}/{source_folder_name}')\r\n",
					"        data_files = self.ls(f'{url}/{source_folder_name}/{rundate_dir}')[1]\r\n",
					"        file_extension = data_files[0].split('.')[1]\r\n",
					"        return batch_type, file_extension        \r\n",
					"\r\n",
					"    def load(self, path):\r\n",
					"        df = spark.read.format('delta').load(self.to_url(path))\r\n",
					"        return df        \r\n",
					"\r\n",
					"    def display(self, path, limit=4):\r\n",
					"        df = spark.read.format('delta').load(self.to_url(path))\r\n",
					"        display(df.limit(limit))\r\n",
					"        return df\r\n",
					"\r\n",
					"    def show(self, path, limit=4):\r\n",
					"        df = spark.read.format('delta').load(self.to_url(path))\r\n",
					"        df.show(limit)\r\n",
					"        return df\r\n",
					"\r\n",
					"    def fix_column_names(self, df):\r\n",
					"        \"\"\" Fix column names to satisfy the Parquet naming requirements by substituting invalid characters with an underscore. \"\"\"\r\n",
					"        df_with_valid_column_names = df.select([F.col(col).alias(self.fix_column_name(col)) for col in df.columns])\r\n",
					"        return df_with_valid_column_names\r\n",
					"\r\n",
					"    def fix_column_name(self, column_name):\r\n",
					"        return re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", column_name) \r\n",
					"\r\n",
					"    def to_spark_schema(self, schema):#: list[list[str]]):\r\n",
					"        \"\"\" Creates a spark schema from a schema specified in the OEA schema format. \r\n",
					"            Example:\r\n",
					"            schemas['Person'] = [['Id','string','hash'],\r\n",
					"                                    ['CreateDate','timestamp','no-op'],\r\n",
					"                                    ['LastModifiedDate','timestamp','no-op']]\r\n",
					"            to_spark_schema(schemas['Person'])\r\n",
					"        \"\"\"\r\n",
					"        fields = []\r\n",
					"        for col_name, dtype, op in schema:\r\n",
					"            fields.append(StructField(col_name, globals()[dtype.lower().capitalize() + \"Type\"](), True))\r\n",
					"        spark_schema = StructType(fields)\r\n",
					"        return spark_schema\r\n",
					"\r\n",
					"    def get_text_from_url(self, url):\r\n",
					"        \"\"\" Retrieves the text doc at the given url. \r\n",
					"            eg: get_text_from_url(\"https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv\")\r\n",
					"        \"\"\"\r\n",
					"        response = urllib.request.urlopen(url)\r\n",
					"        str = response.read().decode('utf-8')  \r\n",
					"        return str\r\n",
					"\r\n",
					"    def parse_metadata_from_csv(self, csv_str):\r\n",
					"        \"\"\" Parses out metadata from a csv string and returns the metadata dictionary. \"\"\"\r\n",
					"        metadata = {}\r\n",
					"        current_entity = ''\r\n",
					"        header = None\r\n",
					"        for line in csv_str.splitlines():\r\n",
					"            line = line.strip()\r\n",
					"            # skip empty lines, lines that start with # (because these are comments), and lines with only commas (which is what happens if someone uses excel and leaves a row blank) \r\n",
					"            if len(line) == 0 or line.startswith('#') or re.match(r'^,+$', line): continue\r\n",
					"            ar = line.split(',')\r\n",
					"\r\n",
					"            if not header:\r\n",
					"                header = []\r\n",
					"                for column_name in ar:\r\n",
					"                    header.append(re.sub(\"[ ,;{}()\\n\\t=]+\", \"_\", column_name))\r\n",
					"                continue\r\n",
					"            \r\n",
					"            # check for the start of a new entity definition\r\n",
					"            if ar[0] != '':\r\n",
					"                current_entity = ar[0]\r\n",
					"                metadata[current_entity] = []\r\n",
					"            # an attribute row must have an attribute name in the second column\r\n",
					"            elif len(ar[1]) > 0:\r\n",
					"                ar = ar[1:] # remove the first element because it will be blank\r\n",
					"                ar[0] = self.fix_column_name(ar[0]) # remove spaces and other illegal chars from column names\r\n",
					"                metadata[current_entity].append(ar)\r\n",
					"            else:\r\n",
					"                logger.info('Invalid metadata row: ' + line)\r\n",
					"        return metadata\r\n",
					"\r\n",
					"    def write(self, data_str, destination_path_and_filename):\r\n",
					"        \"\"\" Writes the given data string to a file on blob storage \"\"\"\r\n",
					"        destination_url = self.to_url(destination_path_and_filename)\r\n",
					"        mssparkutils.fs.put(destination_url, data_str, True) # Set the last parameter as True to create the file if it does not exist\r\n",
					"\r\n",
					"    def get_metadata_from_url(self, url):\r\n",
					"        csv_str = self.get_text_from_url(url)\r\n",
					"        metadata = self.parse_metadata_from_csv(csv_str)\r\n",
					"        return metadata        \r\n",
					"\r\n",
					"    def create_run_date(self):\r\n",
					"        rundate = datetime.datetime.now().replace(microsecond=0) # use UTC for the datetime because when parsing it out later, spark's to_timestamp() assumes the local machine's timezone, and the timezone for the spark cluster will be UTC\r\n",
					"        return rundate\r\n",
					"\r\n",
					"    def land_data(self, data, destination_path, destination_filename, rundate=None):\r\n",
					"        \"\"\" Lands data in the given destination_path, adding a rundate folder.\r\n",
					"        \"\"\"\r\n",
					"        if not rundate: rundate = self.create_run_date()\r\n",
					"        destination_path = f'{destination_path}/rundate={rundate}/{destination_filename}'\r\n",
					"        self.write(data, destination_path)\r\n",
					"        return destination_path\r\n",
					"\r\n",
					"    def land_delta_batch_data(self, data, sink_path, filename, rundate=None):\r\n",
					"        sink_path = self.to_url(f'stage1/Transactional/{sink_path}/delta_batch_data')\r\n",
					"        resulting_path = self.land_data(data, sink_path, filename)\r\n",
					"        return resulting_path \r\n",
					"\r\n",
					"    def land_additive_batch_data(self, data, sink_path, filename, rundate=None):\r\n",
					"        sink_path = self.to_url(f'stage1/Transactional/{sink_path}/additive_batch_data')\r\n",
					"        resulting_path = self.land_data(data, sink_path, filename)\r\n",
					"        return resulting_path\r\n",
					"\r\n",
					"    def land_snapshot_batch_data(self, data, sink_path, filename, rundate=None):\r\n",
					"        sink_path = self.to_url(f'stage1/Transactional/{sink_path}/snapshot_batch_data')\r\n",
					"        resulting_path = self.land_data(data, sink_path, filename)\r\n",
					"        return resulting_path                      \r\n",
					"\r\n",
					"    def upsert(self, df, destination_path, primary_key='id'):\r\n",
					"        \"\"\" Upserts the data in the given dataframe into the specified destination using the given primary_key_column to identify the updates.\r\n",
					"            If there is no delta table found in the destination_path, one will be created.    \r\n",
					"        \"\"\"\r\n",
					"        destination_url = self.to_url(destination_path)\r\n",
					"        df = self.fix_column_names(df)\r\n",
					"        if DeltaTable.isDeltaTable(spark, destination_url):\r\n",
					"            delta_table_sink = DeltaTable.forPath(spark, destination_url)\r\n",
					"            #delta_table_sink.alias('sink').option('mergeSchema', 'true').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll()\r\n",
					"            delta_table_sink.alias('sink').merge(df.alias('updates'), f'sink.{primary_key} = updates.{primary_key}').whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n",
					"        else:\r\n",
					"            logger.debug('No existing delta table found. Creating delta table.')\r\n",
					"            df.write.format('delta').save(destination_url)\r\n",
					"\r\n",
					"    def overwrite(self, df, destination_path):\r\n",
					"        \"\"\" Overwrites the existing delta table with the given dataframe.\r\n",
					"            If there is no delta table found in the destination_path, one will be created.    \r\n",
					"        \"\"\"\r\n",
					"        destination_url = self.to_url(destination_path)\r\n",
					"        df = self.fix_column_names(df)\r\n",
					"        df.write.format('delta').mode('overwrite').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#overwrite        \r\n",
					"\r\n",
					"    def append(self, df, destination_path):\r\n",
					"        \"\"\" Appends the given dataframe to the delta table in the specified destination.\r\n",
					"            If there is no delta table found in the destination_path, one will be created.    \r\n",
					"        \"\"\"\r\n",
					"        destination_url = self.to_url(destination_path)\r\n",
					"        df = self.fix_column_names(df)\r\n",
					"        if DeltaTable.isDeltaTable(spark, destination_url):\r\n",
					"            df.write.format('delta').mode('append').save(destination_url)  # https://docs.delta.io/latest/delta-batch.html#append\r\n",
					"        else:\r\n",
					"            logger.debug('No existing delta table found. Creating delta table.')\r\n",
					"            df.write.format('delta').save(destination_url)\r\n",
					"\r\n",
					"    def process(self, source_path, foreach_batch_function, options={}):\r\n",
					"        \"\"\" This simplifies the process of using structured streaming when processing transformations.\r\n",
					"            Provide a source_path and a function that receives a dataframe to work with (which will be a dataframe with data from the given source_path).\r\n",
					"            Use it like this...\r\n",
					"            def refine_contoso_dataset(df_source):\r\n",
					"                metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/gene/v0.7dev/modules/module_catalog/Student_and_School_Data_Systems/metadata.csv')\r\n",
					"                df_pseudo, df_lookup = oea.pseudonymize(df, metadata['studentattendance'])\r\n",
					"                oea.upsert(df_pseudo, 'stage2/Refined/contoso_sis/v0.1/studentattendance/general')\r\n",
					"                oea.upsert(df_lookup, 'stage2/Refined/contoso_sis/v0.1/studentattendance/sensitive')\r\n",
					"            oea.process('stage2/Ingested/contoso_sis/v0.1/studentattendance', refine_contoso_dataset)             \r\n",
					"        \"\"\"\r\n",
					"        if not self.path_exists(source_path):\r\n",
					"            raise ValueError(f'The given path does not exist: {source_path} (which resolves to: {self.to_url(source_path)})') \r\n",
					"\r\n",
					"        def wrapped_function(df, batch_id):\r\n",
					"            df.persist() # cache the df so it doesn't get read in multiple times when we write to multiple destinations. See: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch\r\n",
					"            foreach_batch_function(df)\r\n",
					"            df.unpersist()\r\n",
					"\r\n",
					"        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
					"        streaming_df = spark.readStream.format('delta').load(self.to_url(source_path), **options)\r\n",
					"        # for more info on append vs complete vs update modes for structured streaming: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\r\n",
					"        query = streaming_df.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', self.to_url(source_path) + '/_checkpoints').foreachBatch(wrapped_function).start()\r\n",
					"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"        number_of_new_inbound_rows = query.lastProgress[\"numInputRows\"]\r\n",
					"        logger.info(f'Number of new inbound rows processed: {number_of_new_inbound_rows}')\r\n",
					"        logger.debug(query.lastProgress)\r\n",
					"        return number_of_new_inbound_rows\r\n",
					"\r\n",
					"    def ingest(self, source_path, primary_key='id', options={}):\r\n",
					"        \"\"\" Ingests all the entities in the given source_path, or ingests the data for the entity if given an entity path.\r\n",
					"            CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\r\n",
					"            To specify options that are different from these defaults, use the options param.\r\n",
					"            eg, ingest('contoso_sis/v0.1') # ingests all entities found in that path\r\n",
					"            eg, ingest('contoso_sis/v0.1/studentattendance') # ingests the batch data for the single entity in this path\r\n",
					"            eg, ingest('contoso_sis/v0.1', options={'header':False}) # for CSV files that don't have a header\r\n",
					"            eg, ingest('contoso_sis/v0.1/studentattendance', options={'multiline':True}) # for JSON files that are not a JSON doc per row\r\n",
					"        \"\"\"\r\n",
					"        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
					"        source_dict = self.parse_path(f'stage1/Transactional/{source_path}')\r\n",
					"        if source_dict['entity']:\r\n",
					"            number_of_new_inbound_rows = self._ingest_entity(source_dict, primary_key, options)\r\n",
					"        else:\r\n",
					"            number_of_new_inbound_rows = 0\r\n",
					"            for name in source_dict['entity_list']:\r\n",
					"                number_of_new_inbound_rows += self._ingest_entity(source_dict, primary_key, options)\r\n",
					"        return number_of_new_inbound_rows\r\n",
					"\r\n",
					"    def _ingest_entity(self, source_dict, primary_key, options):\r\n",
					"        \"\"\" Performs the basic data ingestion - ingesting incoming batch data from stage1 into delta lake format in stage2. \"\"\"\r\n",
					"        destination_path = f'stage2/Ingested/{source_dict[\"between_path\"]}/{source_dict[\"entity\"]}'\r\n",
					"        batch_type, source_data_format = self.get_batch_info(source_dict['entity_path'])\r\n",
					"        logger.info(f'Ingesting from: {source_dict[\"entity_path\"]}, batch type of: {batch_type}, source data format of: {source_data_format}')\r\n",
					"        source_url = self.to_url(f'{source_dict[\"entity_path\"]}/{batch_type}_batch_data')\r\n",
					"\r\n",
					"        if batch_type == 'snapshot': source_url = f'{source_url}/{self.get_latest_folder(source_url)}' \r\n",
					"            \r\n",
					"        logger.debug(f'Processing {batch_type} data from: {source_url} and writing out to: {destination_path}')\r\n",
					"        if batch_type == 'snapshot':\r\n",
					"            def batch_func(df): self.overwrite(df, destination_path)\r\n",
					"        elif batch_type == 'additive':\r\n",
					"            def batch_func(df): self.append(df, destination_path)\r\n",
					"        elif batch_type == 'delta':\r\n",
					"            def batch_func(df): self.upsert(df, destination_path, primary_key)\r\n",
					"        else:\r\n",
					"            raise ValueError(\"No valid batch folder was found at that path (expected to find a single folder with one of the following names: snapshot_batch_data, additive_batch_data, or delta_batch_data). Are you sure you have the right path?\")                      \r\n",
					"\r\n",
					"        if options == None: options = {}\r\n",
					"        options['format'] = source_data_format # eg, 'csv', 'json'\r\n",
					"        if source_data_format == 'csv' and (not 'header' in options or options['header'] == None): options['header'] = True  # default to expecting a header in csv files\r\n",
					"\r\n",
					"        number_of_new_inbound_rows = self.process(source_url, batch_func, options)\r\n",
					"        if number_of_new_inbound_rows > 0:    \r\n",
					"            self.add_to_lake_db(destination_path)\r\n",
					"        return number_of_new_inbound_rows\r\n",
					"\r\n",
					"    def query(self, source_path, query_str, criteria_str=None):\r\n",
					"        df = self.load(source_path)\r\n",
					"        sqlContext.registerDataFrameAsTable(df, 'tmp_source_table')\r\n",
					"        if criteria_str:\r\n",
					"            query = f'{query_str} from tmp_source_table where {criteria_str}'\r\n",
					"        else:\r\n",
					"            query = f'{query_str} from tmp_source_table'\r\n",
					"        df = sqlContext.sql(query)\r\n",
					"        return df       \r\n",
					"\r\n",
					"    def get_latest_changes(self, source_path, sink_path):\r\n",
					"        \"\"\" Returns a dataframe representing the changes in the source data based on the max rundate in the sink data. \r\n",
					"            If the sink path is not found, all of the data from the source_path is returned (the assumption is that the sink delta table is being created for the first time).\r\n",
					"        \"\"\"   \r\n",
					"        maxdatetime = None\r\n",
					"        try:\r\n",
					"            sink_df = self.query(sink_path, 'select max(rundate) maxdatetime')\r\n",
					"            maxdatetime = sink_df.first()['maxdatetime']\r\n",
					"        except AnalysisException as e:\r\n",
					"            # This means that there is no delta table at the sink_path yet.\r\n",
					"            # We'll assume that the sink delta table is being created for the first time, meaning that all of the source data should be returned.\r\n",
					"            pass\r\n",
					"\r\n",
					"        changes_df = self.load(source_path)\r\n",
					"        if maxdatetime:\r\n",
					"            # filter the source table for the latest changes (using the max rundate in the destination table as the watermark)\r\n",
					"            changes_df = changes_df.where(f\"rundate > '{maxdatetime}'\")        \r\n",
					"        return changes_df\r\n",
					"\r\n",
					"    def refine(self, source_path, metadata, primary_key):\r\n",
					"        source_path = f'stage2/Ingested/{source_path}'\r\n",
					"        primary_key = self.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
					"        path_dict = self.parse_path(source_path)\r\n",
					"        sink_general_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/general/' + path_dict['entity']\r\n",
					"        sink_sensitive_path = path_dict['entity_parent_path'].replace('Ingested', 'Refined') + '/sensitive/' + path_dict['entity'] + '_lookup'    \r\n",
					"\r\n",
					"        df_changes = self.get_latest_changes(source_path, sink_general_path)\r\n",
					"\r\n",
					"        if df_changes.count() > 0:\r\n",
					"            df_pseudo, df_lookup = self.pseudonymize(df_changes, metadata)\r\n",
					"            self.upsert(df_pseudo, sink_general_path, f'{primary_key}_pseudonym') # todo: remove this assumption that the primary key will always be hashed during pseduonymization\r\n",
					"            self.upsert(df_lookup, sink_sensitive_path, primary_key)    \r\n",
					"            self.add_to_lake_db(sink_general_path)\r\n",
					"            self.add_to_lake_db(sink_sensitive_path)\r\n",
					"            logger.info(f'Updated rows processed into Refined table: {df_changes.count()}')\r\n",
					"        else:\r\n",
					"            logger.info('No updated rows in Ingested table to process.')\r\n",
					"\r\n",
					"    def load_csv(self, source_path, header=True):\r\n",
					"        \"\"\" Loads a csv file as a dataframe based on the path specified \"\"\"\r\n",
					"        options = {'format':'csv', 'header':header}\r\n",
					"        df = spark.read.load(self.to_url(source_path), **options)\r\n",
					"        return df      \r\n",
					"\r\n",
					"    def load_json(self, source_path, multiline=False):\r\n",
					"        \"\"\" Loads a json file as a dataframe based on the path specified \"\"\"\r\n",
					"        options = {'format':'json', 'multiline':multiline}\r\n",
					"        df = spark.read.load(self.to_url(source_path), **options)\r\n",
					"        return df    \r\n",
					"\r\n",
					"    def pseudonymize(self, df, metadata): #: list[list[str]]):\r\n",
					"        \"\"\" Performs pseudonymization of the given dataframe based on the provided metadata (in the OEA format).\r\n",
					"            For example, if the given df is for an entity called person, \r\n",
					"            2 dataframes will be returned, one called person that has hashed ids and masked fields, \r\n",
					"            and one called person_lookup that contains the original person_id, person_id_pseudo,\r\n",
					"            and the non-masked values for columns marked to be masked.           \r\n",
					"            The lookup table should be written to a \"sensitive\" folder in the data lake.\r\n",
					"            eg, df_pseudo, df_lookup = oea.pseudonymize(df, metadata)\r\n",
					"            [More info on this approach here: https://learn.microsoft.com/en-us/azure/databricks/security/privacy/gdpr-delta#pseudonymize-data]\r\n",
					"        \"\"\"\r\n",
					"        salt = self._get_salt()\r\n",
					"        df_pseudo = df\r\n",
					"        df_lookup = df\r\n",
					"        for row in metadata:\r\n",
					"            col_name = row[0]\r\n",
					"            dtype = row[1]\r\n",
					"            op = row[2]\r\n",
					"            if op == \"hash-no-lookup\" or op == \"hnl\":\r\n",
					"                # This means that the lookup can be performed against a different table so no lookup is needed.\r\n",
					"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
					"                df_lookup = df_lookup.drop(col_name)           \r\n",
					"            elif op == \"hash\" or op == 'h':\r\n",
					"                df_pseudo = df_pseudo.withColumn(col_name, F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256)).withColumnRenamed(col_name, col_name + \"_pseudonym\")\r\n",
					"                df_lookup = df_lookup.withColumn(col_name + \"_pseudonym\", F.sha2(F.concat(F.col(col_name), F.lit(salt)), 256))\r\n",
					"            elif op == \"mask\" or op == 'm':\r\n",
					"                df_pseudo = df_pseudo.withColumn(col_name, F.lit('*'))\r\n",
					"            elif op == \"partition-by\":\r\n",
					"                pass # make no changes for this column so that it will be in both dataframes and can be used for partitioning\r\n",
					"            elif op == \"no-op\" or op == 'x':\r\n",
					"                df_lookup = df_lookup.drop(col_name)\r\n",
					"        return (df_pseudo, df_lookup)\r\n",
					"\r\n",
					"    def add_to_lake_db(self, source_entity_path):\r\n",
					"        \"\"\" Adds the given entity as a table (if the table doesn't already exist) to the proper lake db based on the path.\r\n",
					"            This method will also create the lake db if it doesn't already exist.\r\n",
					"            eg: add_to_lake_db('stage2/Ingested/contoso_sis/v0.1/students')\r\n",
					"\r\n",
					"            Note that a spark db that points to source data in the delta format can't be queried via SQL serverless pool. More info here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake\r\n",
					"        \"\"\"\r\n",
					"        source_dict = self.parse_path(source_entity_path)\r\n",
					"        db_name = source_dict['ldb_name']\r\n",
					"        spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
					"        spark.sql(f\"create table if not exists {db_name}.{source_dict['entity']} using DELTA location '{self.to_url(source_dict['entity_path'])}'\")\r\n",
					"\r\n",
					"    def drop_lake_db(self, db_name):\r\n",
					"        spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\r\n",
					"        result = \"Database dropped: \" + db_name\r\n",
					"        logger.info(result)\r\n",
					"        return result\r\n",
					"\r\n",
					"    def create_sql_db(self, source_path):\r\n",
					"        \"\"\" Prints out the sql script needed for creating a sql serverless db and set of views. \"\"\"\r\n",
					"        source_dict = self.parse_path(source_path)\r\n",
					"        db_name = source_dict['sdb_name']\r\n",
					"        cmd = '-- Create a new sql script then execute the following in it:\\n'\r\n",
					"        cmd += f\"IF NOT EXISTS (SELECT * FROM sys.databases WHERE name = '{db_name}')\\nBEGIN\\n  CREATE DATABASE {db_name};\\nEND;\\nGO\\n\"\r\n",
					"        cmd += f\"USE {db_name};\\nGO\\n\\n\"\r\n",
					"        cmd += self.create_sql_views(source_dict['entity_parent_path'])\r\n",
					"        print(cmd)\r\n",
					"\r\n",
					"    def create_sql_views(self, source_path):\r\n",
					"        cmd = ''      \r\n",
					"        dirs = self.get_folders(source_path)\r\n",
					"        for table_name in dirs:\r\n",
					"            cmd += f\"CREATE OR ALTER VIEW {table_name} AS\\n  SELECT * FROM OPENROWSET(BULK '{self.to_url(source_path)}/{table_name}', FORMAT='delta') AS [r];\\nGO\\n\"\r\n",
					"        return cmd \r\n",
					"\r\n",
					"    def drop_sql_db(self, db_name):\r\n",
					"        cmd = '-- Create a new sql script then execute the following in it. Alternatively, you can click on the menu next to the SQL db and select \"Delete\"\\n'\r\n",
					"        cmd += '-- [Note that this does not affect the data in the data lake - this will only delete the sql db that points to that data.]\\n\\n'\r\n",
					"        cmd += f'DROP DATABASE {db_name}'\r\n",
					"        print(cmd)       \r\n",
					"\r\n",
					"class DataLakeWriter:\r\n",
					"    def __init__(self, root_destination):\r\n",
					"        self.root_destination = root_destination\r\n",
					"\r\n",
					"    def write(self, path_and_filename, data_str, format='csv'):\r\n",
					"        mssparkutils.fs.append(f\"{self.root_destination}/{path_and_filename}\", data_str, True) # Set the last parameter as True to create the file if it does not exist\r\n",
					"\r\n",
					"oea = OEA()"
				],
				"execution_count": null
			}
		]
	}
}